{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d656b465",
   "metadata": {},
   "source": [
    "# Worksheet 17: RL on Robotics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83198b8",
   "metadata": {},
   "source": [
    "**Learning outcomes**\n",
    "1. understand how to create a simple Robot environment that links to Gazebo\n",
    "1. understand how to deal with the simulated environment in a grid world fashion\n",
    "1. appreciate the intricacy of applying RL to the robotics domain\n",
    "1. build on previous concepts to come up with a suitable solution to a problem at hand\n",
    "1. understand how a replay buffer helps us to come closer to supervised learning and appreciate the important role it plays in reaching convergence\n",
    "1. understand how to combine deep reinforcement learning with deep learning to create a powerful framework that allows automatic agent learning by observation or self-play.\n",
    "1. understand how a replay buffer helps us to come closer to supervised learning and appreciate the important role it plays in reaching convergence for difficult problems that involve image processing and reinforcement learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f5f0e1",
   "metadata": {},
   "source": [
    "In this worksheet, we deal with how to set up a robot environment class that can handle the publish-subscribe on topics and deal with services in ROS. We must have ROS and Gazebo installed and set up on our machine. The code is a starting point and is not fully developed. You will need to write the necessary functionality to address a specific requirement. The main idea of tackling robotics applications in a Jupyter notebook is to utilise the provided infrastructure and libraries of code we covered in earlier units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923499de-54e3-4c00-8e35-77a20c2fdde0",
   "metadata": {},
   "source": [
    "## Instructions for running Experiments on Azure VM\n",
    "The VM usage limit is set to 120 hours. Please turn off the machine when not using it to preserve your time. The VM is not set to disconnect you automatically so that you can leave it training the robot continuously for assessment 2.\n",
    "\n",
    "Please turn off the screen save and screen lock in Xfce(Applications->Settings->Light Locker) as it may cause the machine to become not responsive, which in turn, causes Azure to stop it automatically.\n",
    "\n",
    "If the VM becomes corrupted for some reason, then you can reimage it by going to Azure Lab page and selecting the three dots, then reimage. *Reimage will reset the VM to its initial settings but it causes all data you have on the machine to be lost.* You are advised to backup your data, you may want to use OneDrive or other backup methods.\n",
    "\n",
    "If Gazebo stops for any reason, the provided code has a `try-except` statement (in the base MRP class) that you can activate (comment in). It allows you to continue training even if the robot becomes not responsive without having to restart the experiment. But you will need to restart the kernel if you change the undrlying library. \n",
    "\n",
    "You can restart your notebook kernel when you want to re-establish a connection with the environment. However, this is rarely needed and even if gazebo stopped, you can still relaunch usng the above command and just rerun the latest cell wihout having to restart the kernel (let alone Clear All).\n",
    "\n",
    "If you are running out of time, please let your tutor know in advance and they will try to increase your VM time allowance.\n",
    "\n",
    "\n",
    "### Using your own VM\n",
    "You *might* need to do `export PATH=\"home/rl/.local/bin:/opt/ros/foxy/share:$PATH` if you are using your own independent VM.\n",
    "\n",
    "Go to /opt/ros/foxy/share/turtlebot3_gazebo/models/turtlebot3_burger/model.sdf and adjust the <update_rate>30</update_rate> to <update_rate>5</update_rate> for the odomotry turtlebot3_diff_drive. This will change its frequency from 30 to 5 so that it aligns with the scanner frequency.\n",
    "\n",
    "Install [turtlebot3](https://emanual.robotis.com/docs/en/platform/turtlebot3/quick-start/#install-dependent-ros-packages-1) packages. If you are in our VM, the packages are already installed and no need to do anything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d58b74",
   "metadata": {},
   "source": [
    "## Launching the Gazebo Environment with TurtleBot Spawned\n",
    "To launch a Gazebo environment with TurtleBot3, open a terminal and run the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7ffb8",
   "metadata": {},
   "source": [
    "- `ros2 launch turtlebot3_gazebo turtlebot3_house.launch`\n",
    "- `ros2 launch turtlebot3_gazebo turtlebot3_simple.launch.py`\n",
    "- `ros2 launch turtlebot3_gazebo turtlebot3_assessment2.launch.py`\n",
    "\n",
    "To make the testing smoother, you can right-click Gazebo and keep the window on top.\n",
    "You can also press ctrl+R to reset the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84259c",
   "metadata": {},
   "source": [
    "Note that you cannot do that inside the notebook because that the command blocks the notebook from executing other code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e001f029",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe5a1b7",
   "metadata": {},
   "source": [
    "## The RobEnv Class (Interfacing Gazebo with RL Lib)\n",
    "\n",
    "We have set up an environment called `RobEnv` (inside env.robot.py) that allows us to use the algorithms we developed in earlier units directly.\n",
    "\n",
    "So long as the /scan(LaserScan), /odom (Odometry) and /cmd_vel(Twist) topics are available (due to launching a gazebo sim with an env and robot spawned), the environment should work fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf3216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.robot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce824cf",
   "metadata": {},
   "source": [
    "### Summary of the RobEnv Class\n",
    "\n",
    "-`RobEnv`: A ROS2 node that connects a TurtleBot robot with a Gazebo simulation. It manages robot control, sensor readings, and interactions within the environment.\n",
    "\n",
    "-`odom`: Processes odometry data, updating the robot's position (`x, y`) and orientation (`θ`).  \n",
    "-`scan`: Reads laser scan data, replacing infinite values with the maximum sensor range.  \n",
    "-`yaw`: Converts quaternion orientation into a yaw angle in radians.  \n",
    "\n",
    "-`θgoal`: Computes the angular distance between the robot and a goal.  \n",
    "-`distgoal`: Calculates the Euclidean distance to the closest goal.  \n",
    "-`atgoal`: Checks if the robot has reached a goal.  \n",
    "-`atwall`: Detects potential collisions based on laser scan data.  \n",
    "\n",
    "-`reward`: Assigns a reward based on the robot's state and action. It encourages movement towards goals and penalises collisions or undesired actions. If the robot collides with a wall, it automatically resets the environment.  \n",
    "\n",
    "-`s_`: Converts the robot’s real-world position and orientation into a discrete state representation, mapping it to a grid system.  \n",
    "\n",
    "-`spin_n`: Ensures the node updates by calling `ros.spin_once` multiple times.  \n",
    "-`control`: Publishes movement commands.  \n",
    "-`step`: Executes a specified action (`forward, turn left, turn right`), computes the next state, and returns a reward.  \n",
    "-`stop`: Halts all movement.  \n",
    "\n",
    "-`reset`: Restarts the simulation, ensuring a fresh environment for new episodes.  \n",
    "\n",
    "This class is particularly useful for reinforcement learning applications, allowing the TurtleBot to learn navigation behaviours by interacting with a simulated Gazebo environment.  \n",
    "\n",
    "\n",
    "\n",
    "**You can override any of the functions by inheriting the class**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112bcba9",
   "metadata": {},
   "source": [
    "## Setting the #laser scans for the LiDAR Sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6b60e",
   "metadata": {},
   "source": [
    "We have provided you with a code to adjust the number of LiDAR beams in the model.sdf file directly in your jupyter notebook.\n",
    "\n",
    "**Important**: if you changed the number of LiDARs then you will need to relaunch the environmnet for the canhes to take effect, otherwise you will get an error when you run the RL algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41172a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_nscans_LiDAR(nscans=64)\n",
    "accelerate_sim(speed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2890c11",
   "metadata": {},
   "source": [
    "Now we can launch the gazebo environment either \n",
    "\n",
    "1. programmatically using load_gazebo() which means that gazebo will terminate if you interrupted your code for any reason\n",
    "2. or by executing `ros2 launch turtlebot3_gazebo turtlebot3_assessment2.launch.py` in a terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f60722",
   "metadata": {},
   "source": [
    "To ensure that all gazebo process are terminated, execute the following command. This is useful when your environment is stuck with `world client service...` message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f7a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kill_sim_processes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece6b480",
   "metadata": {},
   "source": [
    "Then relaunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee39922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_gazebo() # this not recommended do it from the terminal by executing the line below\n",
    "# ros2 launch turtlebot3_gazebo turtlebot3_world.launch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19491b5c",
   "metadata": {},
   "source": [
    "### Connecting Gazebo with this Notebook\n",
    "To establish the connection between gazebo and this notebook, we ned to run the fllowing command **once**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27480d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ros.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5833b2d",
   "metadata": {},
   "source": [
    "### Moving the robot(Actions)\n",
    "We can now start controlling the robot. Let us look at simple examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472721ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RobEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc0277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56a26fc",
   "metadata": {},
   "source": [
    "As we can see, the turtlebot robot moved forward in gazebo sim which proved that our interface class is working well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42232343-488a-4875-ac74-91f0044984dd",
   "metadata": {},
   "source": [
    "## Rotational and Translational Calibration\n",
    "\n",
    "Let us calibrate the rotational and translational movements of our robot settings. The idea here is to be able to get a consistent behaviour where a robot can consistently complete a full circle in a specified number of times *most of the times*. This is a trial and error process, we usually need to experiment with different settings, bearing in minde the accuracy and efficiency of the robot training that will take place later.\n",
    "\n",
    "The frequency plays an important role as it specifies how many times the velocity changes commands are going to be executed per seconds. This is via our subscription to the /cmd_vel topic and the create_timer() function of the Node class.\n",
    "The second important factor is the number of times the spin_once() is going to be executed. Spining a few times after publishing a command helps stablise the behaviour and gives us more consistency because it helps flush any delayed execution as well as any delayed subscription due to the robot hardware limitation which is simulated to an extent in Gazebo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a1d31-43cc-48c0-ac1c-49f6d537c64d",
   "metadata": {},
   "source": [
    "### Rotation in place to form a full $2\\pi$ circle\n",
    "You could try to increase the θspeed but that will result in more slippage.\n",
    "It is also possible to increase the speed of execution (rather that the speed of the robot) by playing with n which is the number of times a spin_once() is executed. You could also speed up the clock by increasing the hz (frequency) of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9a64cb-95b4-460a-8bb9-0f11ecf4ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "θspeed = pi/3.5\n",
    "speed = 2.0\n",
    "n = 6\n",
    "\n",
    "env = RobEnv(speed=speed, θspeed=θspeed, n=n, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab94915-119e-4796-9174-2ba150ce3ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_test(env):\n",
    "    env.reset()\n",
    "    for _ in range(16):\n",
    "        env.step(0)\n",
    "\n",
    "# %time rotate_test(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61dbca2",
   "metadata": {},
   "source": [
    "Note that each time you run the above code, the robot's final position may vary slightly due to factors such as friction and other physical world influences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b4dc7-35a9-4bf7-9e15-72e8b4945e37",
   "metadata": {},
   "source": [
    "### Translation calibration, moving in a straight line\n",
    "You could try to increase the speed but that will result in bending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e174d-bcac-450b-86ad-b77a1677401c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def forward_test(env):\n",
    "    env.reset()\n",
    "    for _ in range(10):\n",
    "        env.step()\n",
    "\n",
    "# forward_test(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29eb5b-38ef-4571-9372-d577cf5c8e29",
   "metadata": {},
   "source": [
    "### Manual solution (policy) to the given problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbb1b6f-6d70-4ce9-ad4d-86fda40f47fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def optimal_policy1(env):\n",
    "    env.reset( )\n",
    "    for _ in range(2): env.step(0)\n",
    "    for _ in range(8): env.step()\n",
    "    \n",
    "# %time optimal_policy1(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a1ab05-88e3-412e-afc5-cf216a06c7f4",
   "metadata": {},
   "source": [
    "# Applying an RL Algorithms to Train a Turtlebot3 to Autonomously Reach the Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c73384-1f1e-46db-8f99-49d06bcb6ad4",
   "metadata": {},
   "source": [
    "## Vectorised Environment\n",
    "Let us now try to changed teh states from a number/index into vector.\n",
    "We will simply utilise the laser scans. We can use them as is or try to turn them into some form of a hot encoding or tile coding. Below we show a simple implementaiton which you can build on. Note that we will import algorithms from RLv instead of RL so that we can use the vectorised linear model RL algorithms such Sarsa and Q_learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78f56ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out the line below once you installed torch\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6042d418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.rlln import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e33148-992e-4d81-8cce-8e4a0e6d53e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vRobEnv(RobEnv):\n",
    "    def __init__(self, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.nF = len(self.scans)\n",
    "        print('state size(laser beams)=', self.nF)\n",
    "\n",
    "    # overridding reward_, \n",
    "    # you may use goal_dist, Δgoal_dist, θgoal_dist, Δθgoal_dist or at_wall and at_goal\n",
    "    def reward_(self, a):\n",
    "        # reward = sum([-self.Δgoal_dist*(a==1), .1*(a==1), .1*(a!=1),-self.Δθgoal_dist*(a!=1)/pi, -2*self.at_wall, 5*self.at_goal])\n",
    "        # reward = sum([-1, -1*(a != 1)*(not self.at_goal), 1*self.at_goal])/3  #-1*self.at_wall,\n",
    "        reward = sum([-1, 10*self.at_goal])\n",
    "        if self.verbose and reward>-1: print('reward =', reward)#; print(f'action = {a}')\n",
    "        return reward\n",
    "    \n",
    "    # overriding state representation, you may only use the laser self.scans\n",
    "    def s_(self):\n",
    "        max, min = self.max_range, self.min_range\n",
    "        # returns a normalise and descritised componenets\n",
    "        return  1*(((self.scans - min)/(max - min))>=.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2067951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "venv = vRobEnv(speed=speed, θspeed=θspeed, n=n,verbose=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ed577",
   "metadata": {},
   "source": [
    "Now it is time to apply Sarsa on robotics! Note that this might not generate a useful policy yet. You must adjust the above code and tune your RL method hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a67882",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqlearn = Qlearn(env=venv, α=1e-4, q0=0, ε=.0, \\\n",
    "                 max_t=1000, episodes=100, \\\n",
    "                 self_path='vQlearn_exp',\\\n",
    "                 seed=1, **demoGame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4fdd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqlearn.self_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45578b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vqlearn.W[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275d30f4",
   "metadata": {},
   "source": [
    "### Resume Training and Extend training\n",
    "If training interrupted for any reason (including finishing the assigned number of episodes), you can resume it by passing resume=True and rerun all the above cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3d5df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = False\n",
    "\n",
    "if resume:\n",
    "    vqlearn = Qlearn.selfload(self_path='vQlearn_exp')\n",
    "    vqlearn.env = venv\n",
    "    vqlearn.episodes = 105 # extend sthe number of episodes\n",
    "\n",
    "# saving the object after each episode for retrieval in case of a crash \n",
    "%time vqlearn.interact(resume=resume, save_ep=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc15372",
   "metadata": {},
   "source": [
    "### To run without Training do the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e786cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vqlearn.episodes = 200\n",
    "# %time vqlearn.interact(resume=resume, train=False, save_ep=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba49b24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# venv.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed04323",
   "metadata": {},
   "source": [
    "## Nonlinear Function Approximation in Robotics\n",
    "\n",
    "To handle nonlinear function approximation, we provide you with a powerful library, rl.rlnn, which includes a set of classes that can be readily utilised, including for implementing DQN. This library offers the essential infrastructure required to implement DQN, Double DQN, and other RL algorithms, should you need them.\n",
    "\n",
    "In the following code, we use a fully connected neural network with Q-learning, specifically within the Deep Q-Network (DQN) framework. It is important to note that, despite the name, DQN does not necessarily require a deep architecture—especially when the input consists of simple laser scan data, which is far less complex than high-dimensional images typically handled by convolutional neural networks (CNNs).\n",
    "\n",
    "The model architecture is configurable via the h1 and h2 parameters, which define the sizes of the hidden layers. Setting either (or both) to 0 simplifies the network structure. By default, the setup supports up to four layers, which is generally sufficient for the intended robotics applications.\n",
    "\n",
    "If you wish to experiment with deeper architectures, you can modify the underlying model definition. However, for most cases with low-dimensional inputs, such as laser beams, additional depth is often unnecessary and may lead to overfitting or increased training complexity without significant benefits.\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- `t_Qn`: Frequency (in steps) of updating the target network, used to stabilize learning alongside the main network.\n",
    "- `save_weights`: Determines how often the model's weights are saved to disk, which is useful for resuming training after interruptions.\n",
    "- `nbatch`: Size of the *mini-batch* sampled from the experience replay buffer for training.\n",
    "- `nbuffer`: Minimum number of experiences required in the *replay buffer* before learning commences.\n",
    "- `h1`: Size of the first hidden layer (set to `0` for no hidden layer).\n",
    "- `h2`: Size of the second hidden layer (also settable to `0` if not needed).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You might consider enhancing the complexity of the environment's state representation to better match the expressive capacity of the neural network, particularly when using a Deep Q-Network (DQN) framework. A more detailed or structured input can help the model leverage the network's architecture more effectively, leading to better learning outcomes and more robust policy development.\n",
    "\n",
    "For instance, increasing the resolution or range of the laser scan input can align more closely with the neural network’s ability to process and learn from richer data. While a small number of beams may suffice for basic reactive behaviors, expanding the laser scan to include a denser or wider set of measurements provides more spatial context, allowing the model to learn more nuanced and sophisticated policies.\n",
    "\n",
    "This adjustment should remain computationally efficient and avoids the complexities of integrating visual data, which is not the focus in this case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1748d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.rlnn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab24464",
   "metadata": {},
   "source": [
    "Below is a skeleton of the `nnRobEnv` class intended to be adapted for use with a non-linear function approximation model. Feel free to modify it as needed to suit your specific implementation, at the moment is uses the same reward and state of the `vRobEnv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ad952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnRobEnv(vRobEnv):\n",
    "    def __init__(self, **kw):\n",
    "        super().__init__(**kw)\n",
    "    \n",
    "    # def reward_(self, a):\n",
    "    #     return super().reward_(a)\n",
    "    \n",
    "    def reward_(self, a):\n",
    "        reward = 10*self.at_goal # sparse reward\n",
    "        if self.verbose and reward: print('reward =', reward)\n",
    "        return reward\n",
    "    \n",
    "    def s_(self):\n",
    "        return super().s_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5857fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnenv = nnRobEnv(speed=speed, θspeed=θspeed, n=n, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c5ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnqlearn = DQN(env=nnenv, \\\n",
    "                episodes=100, \\\n",
    "                α=1e-4, ε=0.5, dε=.99, εmin=0.01, γ=.95, \\\n",
    "                h1=0, h2=0, nF=32, \\\n",
    "                nbuffer=5000, nbatch=32, endbatch=8,\\\n",
    "                t_Qn=100, \\\n",
    "                self_path='DQN_exp',\n",
    "                seed=1, **demoGame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cbe202",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = False\n",
    "\n",
    "if resume:\n",
    "    nnqlearn = DQN.selfload(self_path='DQN_exp')\n",
    "    nnqlearn.env = nnenv\n",
    "    nnqlearn.episodes = 310 # change episodes as needed\n",
    "\n",
    "# saving the object after each episode for retrieval in case of a crash \n",
    "%time nnqlearn.interact(resume=resume, save_ep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b82678",
   "metadata": {},
   "source": [
    "## Loading an Algorithm Object after a Crash\n",
    "If the whole VM stopped for any reasons, and you have lost your local variables in the Jupyter notebook, then you can resume, by loading the latest saved object.\n",
    "\n",
    "To see how, you can Restart the kernel in the notebook which will delete all variable and imported libraries, then you can set resume = True and run all previous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2e94b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in nnqlearn.qN.layers:\n",
    "    print(layer.weight)\n",
    "    # print(layer.bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3afe1e",
   "metadata": {},
   "source": [
    "### Measuring Success Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267147ba",
   "metadata": {},
   "source": [
    "If you have accelerated the simulation speed in Gazebo using set_real_time_rate, make sure to restore both the real-time rate and step size to a more standard values before measuring success rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89085cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate_sim(1) # back to normal\n",
    "kill_sim_processes() # need to reload the sim for the change to take effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb529c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnqlearn.episodes = 300\n",
    "%time nnqlearn.interact(resume=True, train=False, save_ep=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbaf881",
   "metadata": {},
   "source": [
    "## Training Headless\n",
    "To train more efficiently, you can disable the GUI in Gazebo. To do this, locate the .launch file that you use to launch Gazebo and comment out the following lines:\n",
    "\n",
    "       IncludeLaunchDescription(\n",
    "           PythonLaunchDescriptionSource(\n",
    "               os.path.join(pkg_gazebo_ros, 'launch', 'gzclient.launch.py')\n",
    "           ),\n",
    "       ),\n",
    "\n",
    "However, be aware that turning off the GUI may cause the robot to become unstable due to factors like wall collisions or lag. Without the visual feedback, it can be challenging to monitor the robot’s behaviour. If this happens and you need to see the robot’s state, you can press Ctrl + R to reset the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d12c61c",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this worksheet, we have covered using RobEnv class and . We saw how to establish a connection between our notebook and the Gazebo simulated environment to control and train a robot inside it via one of our RL algorithms that we estbalished in earlier units."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
