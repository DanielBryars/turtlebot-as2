{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6750369",
   "metadata": {},
   "source": [
    "#Markdown\n",
    "\n",
    "RL/\n",
    "├── env/\n",
    "│   ├── grid.py\n",
    "│   ├── gridln.py\n",
    "│   ├── gridnn.py\n",
    "│   ├── mountainln.py\n",
    "│   ├── robot.py           ← Gazebo interface and Environment\n",
    "│   └── robot_old.py\n",
    "├── rl/\n",
    "│   ├── dp.py              ← Dynamic programming\n",
    "│   ├── rl.py              ← Core RL logic\n",
    "│   ├── rlln.py            ← Linear approximation model\n",
    "│   ├── rlnn.py            ← Non-linear model \n",
    "│   └── rlselect.py        ← \"Runs\" code for running experimental trials comparing  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e4fbee",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cd99e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from env.robot import *\n",
    "from env.robot import *\n",
    "import numpy as np\n",
    "from math import pi\n",
    "from time import sleep\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a2fcc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3080 Ti\n"
     ]
    }
   ],
   "source": [
    "#check on the GPU support\n",
    "import torch\n",
    "print(torch.cuda.is_available())  # Should be True\n",
    "print(torch.cuda.get_device_name(0))  # Should show \"RTX 3080 Ti\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d57f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_robot_state(env: RobEnv):\n",
    "    #note: env.x and env.y are rounded to 1dp\n",
    "    print (f\"Odom. Pos:[{env.x},{env.y}] Yaw:{env.θ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391642c5",
   "metadata": {},
   "source": [
    "# Connect to ROS\n",
    "\n",
    "1) Launch simulation environment\n",
    "2) init ros (connect to ROS DDS eventing)\n",
    "3) create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49143321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed  =  2.0\n",
      "θspeed =  0.9\n",
      "Odom. Pos:[0,0] Yaw:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Start Gazebo\n",
    "# ros2 launch turtlebot3_gazebo turtlebot3_world.launch.py\n",
    "\n",
    "if not ros.ok():\n",
    "    ros.init()\n",
    "\n",
    "\n",
    "θspeed = pi/3.5\n",
    "speed = 2.0\n",
    "n = 6\n",
    "\n",
    "env = RobEnv(speed=speed, θspeed=θspeed, n=n, verbose=True)\n",
    "\n",
    "print_robot_state(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5f93d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reset env\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "482132f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odom. Pos:[0.1,-0.0] Yaw:0.0\n"
     ]
    }
   ],
   "source": [
    "#Test connection\n",
    "\n",
    "\n",
    "env.step()\n",
    "print_robot_state(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b99e7",
   "metadata": {},
   "source": [
    "# Model 1: Action-value with linear function approximation\n",
    "\n",
    "RL method explanation + justification\n",
    "\n",
    "State representation\n",
    "\n",
    "Reward function\n",
    "\n",
    "Hyperparameter tuning\n",
    "\n",
    "Learning curves + discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969a778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce587cf",
   "metadata": {},
   "source": [
    "Model 2: Either policy gradient or value-based with non-linear function approximation\n",
    "\n",
    "I have also provided you with the ability to easily create a simple, fully connected neural network in the nnMRP class. You do not need to do a lot; the size of the input dictates the choice between a CNN-based model and a fully connected neural network. You may use either in your project, but using a traditional network, not a CNN, is easier and less time-consuming. The CNN-based one is for learning from the pixels of an input image; we are just using the laser reading in our project.\n",
    "\n",
    "\n",
    "\n",
    "I’ve also made slight changes to rl.rl.py to enable storing and retrieving an object (via pickle), which can be useful in case of unexpected crashes—something not uncommon in robotics.\n",
    "\n",
    "Please ensure you download turtlebot3.zip as well to guarantee the environment runs smoothly without any missing files. If you run into any issues, feel free to reach out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6066b",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Núñez, P., Vazquez-Martin, R., Bandera, A., and Romero-Gonzalez, C. (2015) ‘Feature extraction from laser scan data based on curvature estimation for mobile robotics’, *Robotics and Autonomous Systems*, 70, pp. 103–114. Available at: [https://robolab.unex.es/wp-content/papercite-data/pdf/feature-extraction-from-laser.pdf](https://robolab.unex.es/wp-content/publicaciones/2006/Nunez%20Trujillo,%20Vazquez-Martin,%20del%20Toro,%20Bandera%20%7C%20Feature%20extraction%20from%20laser%20scan%20data%20based%20on%20curvature%20estimation%20for%20mobile%20robotics.pdf) (Accessed: 26 April 2025).\n",
    "\n",
    "- Ramos, J., Rocha, R., and Dias, J. (2022) ‘Efficient approach for extracting high-level B-spline features from laser scan data’, *Sensors*, 22(24), 9737. Available at: [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9737135/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9737135/) (Accessed: 26 April 2025).\n",
    "\n",
    "- Shen, S., Michael, N., and Kumar, V. (2012) ‘Method for corner feature extraction from laser scan data’, *ResearchGate*. Available at: [https://www.researchgate.net/publication/288577925_Method_for_corner_feature_extraction_from_laser_scan_data](https://www.researchgate.net/publication/288577925_Method_for_corner_feature_extraction_from_laser_scan_data) (Accessed: 26 April 2025).\n",
    "\n",
    "- Stack Overflow (2019) ‘How can I detect the corner from 2D point cloud or LiDAR scanned data?’, *Stack Overflow*. Available at: [https://stackoverflow.com/questions/59049990/how-can-i-detect-the-corner-from-2d-point-cloud-or-lidar-scanned-data](https://stackoverflow.com/questions/59049990/how-can-i-detect-the-corner-from-2d-point-cloud-or-lidar-scanned-data) (Accessed: 26 April 2025).\n",
    "\n",
    "\n",
    "- CETI. (n.d.) *Simulation Speed in ROS/Gazebo*. Available at: [https://ceti.pages.st.inf.tu-dresden.de/robotics/howtos/SimulationSpeed.html](https://ceti.pages.st.inf.tu-dresden.de/robotics/howtos/SimulationSpeed.html) (Accessed: 26 April 2025).\n",
    "\n",
    "- Furrer, F., Wermelinger, M., Naegeli, T., et al. (2021) ‘Dynamics and Control of Quadrotor UAVs: A Survey’, *IEEE Transactions on Robotics*, 37(5), pp. 1381–1400. Available at: [https://ieeexplore.ieee.org/document/9453594](https://ieeexplore.ieee.org/document/9453594) (Accessed: 26 April 2025).\n",
    "\n",
    "- Perez-Perez, J., Jimenez, F. and Mata, M. (2023) ‘An Overview of Reinforcement Learning in Autonomous Driving: Fundamentals, Challenges, and Applications’, *Applied Sciences*, 13(12), p. 7202. Available at: [https://www.mdpi.com/2076-3417/13/12/7202](https://www.mdpi.com/2076-3417/13/12/7202) (Accessed: 26 April 2025).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbb3939",
   "metadata": {},
   "source": [
    "# Appendicies\n",
    "\n",
    "## Cool links / interesting reading: \n",
    "- https://github.com/hello-robot/stretch_ros/blob/master/stretch_funmap/README.md\n",
    "- https://arxiv.org/pdf/2502.20607\n",
    "\n",
    "## Miscelaneous Notes\n",
    "\n",
    "### Setting up ROS\n",
    "- https://emanual.robotis.com/docs/en/platform/turtlebot3/sbc_setup/\n",
    "- https://ros2-industrial-workshop.readthedocs.io/en/latest/_source/navigation/ROS2-Turtlebot.html\n",
    "- https://emanual.robotis.com/docs/en/platform/turtlebot3/navigation/\n",
    "- https://emanual.robotis.com/docs/en/platform/turtlebot3/bringup/#bringup\n",
    "\n",
    "### Multicast traffic (for DDS) through Windows FW to WSL2:\n",
    "- https://eprosima-dds-router.readthedocs.io/en/latest/rst/examples/repeater_example.html#execute-example\n",
    "- New-NetFirewallRule -Name 'WSL' -DisplayName 'WSL' -InterfaceAlias 'vEthernet (WSL (Hyper-V firewall))' -Direction Inbound -Action Allow\n",
    "- New-NetIPAddress -InterfaceAlias 'vEthernet (WSL (Hyper-V firewall))' -IPAddress '192.168.1.217' -PrefixLength 24\n",
    "- https://github.com/DanielBryars/multicast-test.git\n",
    "\n",
    "### VM\n",
    "- https://labs.azure.com/virtualmachines?feature_vnext=true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
