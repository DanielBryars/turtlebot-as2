{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6750369",
   "metadata": {},
   "source": [
    "#Markdown\n",
    "\n",
    "```plaintext\n",
    "RL/\n",
    "├── env/\n",
    "│   ├── grid.py\n",
    "│   ├── gridln.py\n",
    "│   ├── gridnn.py\n",
    "│   ├── mountainln.py\n",
    "│   ├── robot.py           ← Gazebo interface and Environment\n",
    "│   └── robot_old.py\n",
    "├── rl/\n",
    "│   ├── dp.py              ← Dynamic programming\n",
    "│   ├── rl.py              ← Core RL logic\n",
    "│   ├── rlln.py            ← Linear approximation model\n",
    "│   ├── rlnn.py            ← Non-linear model \n",
    "│   └── rlselect.py        ← \"Runs\" code for running experimental trials comparing\n",
    "│\n",
    "├── Assessment2.ipynb      ← This code part of submission\n",
    "├── robot_environment.py   ← vRobEnv includes state representation and reward structure\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e4fbee",
   "metadata": {},
   "source": [
    "# Imports / Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd99e8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrobot_environment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrlnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "from env.robot import *\n",
    "import numpy as np\n",
    "from math import pi\n",
    "from time import sleep\n",
    "#from tqdm import tqdm\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "import termios\n",
    "import tty\n",
    "import select\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from robot_environment import *\n",
    "from rl.rlnn import *\n",
    "\n",
    "#ACTIONS - make the code easier to read\n",
    "FORWARDS = 1\n",
    "LEFT = 0\n",
    "RIGHT = 2\n",
    "\n",
    "#Really want it deterministic\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#GPU support?\n",
    "print(torch.cuda.is_available()) \n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "309a420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to nuke write protection for these files\n",
    "#sudo chmod 777 /opt/ros/humble/share/turtlebot3_gazebo/worlds/turtlebot3_assessment2/burger.model\n",
    "#sudo chmod 777 /opt/ros/humble/share/turtlebot3_gazebo/models/turtlebot3_burger/model.sdf\n",
    "\n",
    "accelerate_sim(speed=100)\n",
    "set_nscans_LiDAR(nscans=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44890c0",
   "metadata": {},
   "source": [
    "### Common / Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d57f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_robot_odom(env: RobEnv):\n",
    "    #note: env.x and env.y are rounded to 1dp\n",
    "    print (f\"Odom. Pos:[{env.x},{env.y}] Yaw:{env.θ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391642c5",
   "metadata": {},
   "source": [
    "# Connect to ROS / Configure Environment\n",
    "\n",
    "1) Launch simulation environment\n",
    "2) init ros (connect to ROS DDS eventing)\n",
    "3) create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49143321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num laser scans in sensor:'360'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Start Gazebo\n",
    "# Assessment world:\n",
    "# ros2 launch turtlebot3_gazebo turtlebot3_assessment2.launch.py\n",
    "\n",
    "# Other worlds\n",
    "# ros2 launch turtlebot3_gazebo turtlebot3_simple.launch.py \n",
    "# ros2 launch turtlebot3_gazebo empty_world.launch.py\n",
    "# ros2 launch turtlebot3_gazebo turtlebot3_world.launch.py\n",
    "# ros2 launch turtlebot3_gazebo turtlebot3_house.launch.py\n",
    "# rviz2 \n",
    "\n",
    "if not ros.ok():\n",
    "    ros.init()\n",
    "\n",
    "nscans = get_nscans_LiDAR()\n",
    "print(f\"Num laser scans in sensor:'{nscans}'\")\n",
    "# set_nscans_LiDAR(nscans=64)\n",
    "\n",
    "# accelerate_sim(speed=10)\n",
    "# Note running on my machine (AMD Ryzen 9 7950X, 64GB RAM, 3080TI) I got a stable Real Time Factor of 1.00 in the simulation.\n",
    "# I had problems if I set the real time target to more than this, and from reading up this is a knowm limitation in the old Gazebo environment\n",
    "# the new Gazebo Sim properly abstracts time so there is a simulation time independent from wall clock time. This version doesn't play nicely unless it's 1:1\n",
    "# See Simulation Speed in ROS/Gazebo   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f93d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Values (see calibration below for how these values were picked for the most repeatable settings)\n",
    "θspeed = pi/3.5\n",
    "speed = 10.0\n",
    "n = 6\n",
    "\n",
    "env = RobEnv(speed=speed, θspeed=θspeed, n=n, verbose=True)\n",
    "env.reset()\n",
    "print_robot_odom(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "482132f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odom. Pos:[1.2,-0.1] Yaw:6.18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test connection\n",
    "\n",
    "for _ in range(10): env.step()\n",
    "\n",
    "print_robot_odom(env)\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5809e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_test(env, speed=2.0):\n",
    "    env.speed = speed\n",
    "    env.reset()\n",
    "    \n",
    "    steps = 10\n",
    "    for _ in tqdm(range(steps), desc=f\"Moving at speed {speed}\", leave=False):\n",
    "        env.step(FORWARDS)\n",
    "\n",
    "    yDriftPerStep = env.y / steps\n",
    "    xPerStep = env.x / steps\n",
    "    #print(f\"Speed {speed}: y drift = {yDriftPerStep:.4f}, x per step: {xPerStep:.4f}\")\n",
    "\n",
    "    BACKWARDS = -1 #not used in simulation, just used to test repeatability\n",
    "    for _ in tqdm(range(steps), desc=f\"Moving at speed {speed}\", leave=False):\n",
    "        env.step(BACKWARDS)\n",
    "\n",
    "    xDrift = env.x\n",
    "\n",
    "    return yDriftPerStep, xPerStep, xDrift\n",
    "\n",
    "num_trials = 5\n",
    "speeds = [0.5, 1.0, 2.0, 3.0,5.0, 10.0]\n",
    "log = {\"speed\": [], \"trial\": [], \"yDriftPerStep\": [], \"xPerStep\": [], \"xDrift\": []}\n",
    "\n",
    "for speed in tqdm(speeds, desc=\"Speeds\"):\n",
    "    for trial in tqdm(range(num_trials), desc=f\"Trials at speed {speed}\", leave=False):\n",
    "        y_drift, x_step, xDrift = forward_test(env, speed=speed)\n",
    "        log[\"speed\"].append(speed)\n",
    "        log[\"trial\"].append(trial)\n",
    "        log[\"yDriftPerStep\"].append(y_drift)\n",
    "        log[\"xPerStep\"].append(x_step)\n",
    "        log[\"xDrift\"].append(xDrift)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot y drift per step\n",
    "plt.subplot(1, 3, 1)\n",
    "for speed in speeds:\n",
    "    y_drift_vals = [log[\"yDriftPerStep\"][i] for i in range(len(log[\"speed\"])) if log[\"speed\"][i] == speed]\n",
    "    plt.plot(range(num_trials), y_drift_vals, marker='o', label=f\"Speed {speed}\")\n",
    "\n",
    "plt.title('Y Drift per Step across Trials')\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('Y Drift per Step')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot x movement per step\n",
    "plt.subplot(1, 3, 2)\n",
    "for speed in speeds:\n",
    "    x_step_vals = [log[\"xPerStep\"][i] for i in range(len(log[\"speed\"])) if log[\"speed\"][i] == speed]\n",
    "    plt.plot(range(num_trials), x_step_vals, marker='o', label=f\"Speed {speed}\")\n",
    "plt.title('X Movement per Step across Trials')\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('X per Step')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot total x drift\n",
    "plt.subplot(1, 3, 3)\n",
    "for speed in speeds:\n",
    "    x_drift_vals = [log[\"xDrift\"][i] for i in range(len(log[\"speed\"])) if log[\"speed\"][i] == speed]\n",
    "    plt.plot(range(num_trials), x_drift_vals, marker='o', label=f\"Speed {speed}\")\n",
    "plt.title('Total X Drift across Trials')\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('Total X Drift')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f829c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_rotation(robot : RobEnv, nTrials=20, frequency=10, angular_velocity=0.3, nSteps=8):\n",
    "    \n",
    "    robot.freq = frequency\n",
    "    robot.angular_velocity = angular_velocity   \n",
    "    robot.n = nSteps\n",
    "\n",
    "    results  = np.zeros((nTrials, 2)) \n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(nTrials):\n",
    "        robot.reset()\n",
    "\n",
    "        step_count = 0\n",
    "        full_rotation_detected = False\n",
    "        half_rotation_detected = False\n",
    "\n",
    "        pbar = tqdm(desc=\"Calibrating Rotation\", unit=\"step\")\n",
    "        while(not full_rotation_detected):\n",
    "            \n",
    "            step_count += 1\n",
    "            robot.step(LEFT)\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"rotation (°)\": f\"{np.degrees(robot.θ):.2f}\",\n",
    "                \"step\": step_count,\n",
    "                \"elapsed (s)\": f\"{time.time() - start_time:.1f}\"\n",
    "            })\n",
    "\n",
    "            if (robot.θ > pi):\n",
    "                half_rotation_detected = True\n",
    "            if (robot.θ <pi and half_rotation_detected):\n",
    "                full_rotation_detected = True\n",
    "            \n",
    "        #print(f\"step_count:{step_count}: final position: {np.degrees(robot.θ):.2f} degrees\")\n",
    "        results [i,0] = robot.θ\n",
    "        results [i,1] = step_count\n",
    "    \n",
    "    return results\n",
    "\n",
    "#Let's try to find the best parameters for the rotation\n",
    "frequencies = [10,20,30]\n",
    "angular_velocities = [0.3, 0.6, 0.9]\n",
    "nSteps = [8,16,32]\n",
    "\n",
    "for frequency in frequencies:\n",
    "    for angular_velocity in angular_velocities: \n",
    "        for n in nSteps:\n",
    "            results = measure_rotation(env, nTrials=20, frequency=frequency, angular_velocity=angular_velocity, nSteps=n)\n",
    "            angle_per_turn = (2 * np.pi + results[:, 0]) / results[:, 1]\n",
    "            results = np.hstack([results, angle_per_turn[:, None]])\n",
    "\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "            # Plot final errors\n",
    "            ax1.plot(np.degrees(results[:,2]), 'o-')\n",
    "            ax1.set_title(f'Rotation per Step frequency:{frequency} angular_velocity:{angular_velocity} n:{n}')\n",
    "            ax1.set_xlabel('Trial Number')\n",
    "            ax1.set_ylabel('Rotation (degrees)')\n",
    "            ax1.grid(True)\n",
    "\n",
    "            # Plot step counts\n",
    "            ax2.plot(results[:,1], 'o-')\n",
    "            ax2.set_title('Steps Taken vs Trial')\n",
    "            ax2.set_xlabel('Trial Number')\n",
    "            ax2.set_ylabel('Number of Steps')\n",
    "            ax2.grid(True)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'plot_measure_rotation_{frequency}_{angular_velocity}_{n}.png')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b99e7",
   "metadata": {},
   "source": [
    "# Model 1: Action-value with linear function approximation\n",
    "\n",
    "RL method explanation + justification\n",
    "\n",
    "State representation\n",
    "\n",
    "Reward function\n",
    "\n",
    "Hyperparameter tuning\n",
    "\n",
    "Learning curves + discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "969a778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assumptions\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584949bb",
   "metadata": {},
   "source": [
    "## State representation\n",
    "\n",
    "Explain design\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b990031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:90% !important}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3080 Ti\n",
      "speed  =  2.0\n",
      "θspeed =  0.52\n",
      "state size(laser beams)= 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_t': 2000,\n",
       " 'ε': 0.2,\n",
       " 'εmin': 0.05,\n",
       " 'dε': 7.500000000000001e-05,\n",
       " 'α': 0.05,\n",
       " 'γ': 0.99,\n",
       " 'λ': 0.6,\n",
       " 'verbose': False,\n",
       " 'θspeed': 0.5235987755982988,\n",
       " 'speed': 2.0,\n",
       " 'n': 3}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "from env.robot import *\n",
    "import numpy as np\n",
    "from math import pi\n",
    "from time import sleep\n",
    "#from tqdm import tqdm\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "import termios\n",
    "import tty\n",
    "import select\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from robot_environment import *\n",
    "from rl.rlnn import *\n",
    "\n",
    "#ACTIONS - make the code easier to read\n",
    "FORWARDS = 1\n",
    "LEFT = 0\n",
    "RIGHT = 2\n",
    "\n",
    "#Really want it deterministic\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#GPU support?\n",
    "print(torch.cuda.is_available()) \n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0)) \n",
    "\n",
    "\n",
    "if not ros.ok():\n",
    "    ros.init()  \n",
    "\n",
    "max_t = 2000\n",
    "ε = 0.2\n",
    "εmin = 0.05\n",
    "dε = (ε - εmin) / max_t\n",
    "\n",
    "hyperparameters = {\n",
    "    'max_t':max_t,\n",
    "    'ε': ε, #Initial value Used in the epsilon Greedy so it will be random ε times per request for a next action\n",
    "    'εmin': εmin, # epsilon decreases (there's a theory that if this decreases to 0 at infinity you're ll have teh optimum solution)\n",
    "    'dε': dε, # dtop in epsilon per step\n",
    "    'α': 0.05, # learning rate how much of the new informatin is used to update the existing value prediction         \n",
    "    'γ': 0.99, # discount factor, large means that the rewards in the future contribute to the current action state prediction a lot         \n",
    "    'λ': 0.6, # trace decay for the eligibility trace 1 is MC, 0 is just the last step\n",
    "\n",
    "    'verbose': False,\n",
    "    \n",
    "    # Robot Environment params\n",
    "    'θspeed': pi / 2, \n",
    "    'speed': 3.0,        \n",
    "    'n': 3    \n",
    "}\n",
    "\n",
    "env = vRobEnv(\n",
    "    speed=hyperparameters['speed'],\n",
    "    θspeed=hyperparameters['θspeed'],\n",
    "    n=hyperparameters['n'],\n",
    "    verbose=hyperparameters['verbose']\n",
    ")\n",
    "\n",
    "vqlearn = Sarsaλ(\n",
    "    env=env,\n",
    "    α=hyperparameters['α'],          \n",
    "    γ=hyperparameters['γ'],          \n",
    "    λ=hyperparameters['λ'],           \n",
    "    ε=hyperparameters['ε'],            \n",
    "    εmin=hyperparameters['εmin'],        \n",
    "    dε=hyperparameters['dε'],        \n",
    "    q0=0,             \n",
    "    Tstar=0,          \n",
    "    max_t=1200,       \n",
    "    episodes=200,     \n",
    "    self_path='SarsaLambda.three_levels.test009.pkl',\n",
    "    seed=1,\n",
    "    **demoGame()\n",
    ")\n",
    "\n",
    "print(hyperparameters)\n",
    "vqlearn.interact(resume=False, save_ep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88664b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "from env.robot import *\n",
    "import numpy as np\n",
    "from math import pi\n",
    "from time import sleep\n",
    "#from tqdm import tqdm\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "import termios\n",
    "import tty\n",
    "import select\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from robot_environment import *\n",
    "from rl.rlnn import *\n",
    "\n",
    "#ACTIONS - make the code easier to read\n",
    "FORWARDS = 1\n",
    "LEFT = 0\n",
    "RIGHT = 2\n",
    "\n",
    "#Really want it deterministic\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#GPU support?\n",
    "print(torch.cuda.is_available()) \n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0)) \n",
    "\n",
    "\n",
    "if not ros.ok():\n",
    "    ros.init()  \n",
    "\n",
    "max_t = 2000\n",
    "ε = 0.2\n",
    "εmin = 0.05\n",
    "dε = (ε - εmin) / max_t\n",
    "\n",
    "hyperparameters = {\n",
    "    'max_t':max_t,\n",
    "    'ε': ε, #Initial value Used in the epsilon Greedy so it will be random ε times per request for a next action\n",
    "    'εmin': εmin, # epsilon decreases (there's a theory that if this decreases to 0 at infinity you're ll have teh optimum solution)\n",
    "    'dε': dε, # dtop in epsilon per step\n",
    "    'α': 0.05, # learning rate how much of the new informatin is used to update the existing value prediction         \n",
    "    'γ': 0.99, # discount factor, large means that the rewards in the future contribute to the current action state prediction a lot         \n",
    "    'λ': 0.6, # trace decay for the eligibility trace 1 is MC, 0 is just the last step\n",
    "\n",
    "    'verbose': False,\n",
    "    \n",
    "    # Robot Environment params\n",
    "    'θspeed': pi / 2, \n",
    "    'speed': 3.0,        \n",
    "    'n': 3    \n",
    "}\n",
    "\n",
    "env = vRobEnvCornerDetector(\n",
    "    speed=hyperparameters['speed'],\n",
    "    θspeed=hyperparameters['θspeed'],\n",
    "    n=hyperparameters['n'],\n",
    "    verbose=hyperparameters['verbose']\n",
    ")\n",
    "\n",
    "vqlearn = Sarsaλ(\n",
    "    env=env,\n",
    "    α=hyperparameters['α'],          \n",
    "    γ=hyperparameters['γ'],          \n",
    "    λ=hyperparameters['λ'],           \n",
    "    ε=hyperparameters['ε'],            \n",
    "    εmin=hyperparameters['εmin'],        \n",
    "    dε=hyperparameters['dε'],        \n",
    "    q0=0,             \n",
    "    Tstar=0,          \n",
    "    max_t=1200,       \n",
    "    episodes=200,     \n",
    "    self_path='SarsaLambda.three_levels.test009.pkl',\n",
    "    seed=1,\n",
    "    **demoGame()\n",
    ")\n",
    "\n",
    "print(hyperparameters)\n",
    "vqlearn.interact(resume=False, save_ep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bfc771",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3964023741.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[21], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    stop here\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "stop here\n",
    "\n",
    "#set_nscans_LiDAR(nscans=64)\n",
    "#sdfdsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af1030f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.6666666666666666 = SUM (-1,0,0,0.0)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n",
      "reward:-0.3333333333333333 = SUM (-1,0,0,0.2)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[0;32m~/git/turtlebot-as2/rl/rl.py:192\u001b[0m, in \u001b[0;36mMRP.interact\u001b[0;34m(self, train, resume, save_ep, episodes, grid_img, **kw)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 192\u001b[0m rn,sn, a,an, done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# takes a step in env and store tarjectory if needed\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monline(s, rn,sn, done, a,an) \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# to learn online, pass a one step trajectory\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mΣr \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rn\n",
      "File \u001b[0;32m~/git/turtlebot-as2/rl/rl.py:152\u001b[0m, in \u001b[0;36mMRP.step_an\u001b[0;34m(self, s, a, t)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_an\u001b[39m(\u001b[38;5;28mself\u001b[39m, s,a, t):                          \n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipstep: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     sn, rn, done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     an \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy(sn)\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# we added s=s for compatibility with deep learning later\u001b[39;00m\n",
      "File \u001b[0;32m~/git/turtlebot-as2/env/robot.py:217\u001b[0m, in \u001b[0;36mRobEnv.step\u001b[0;34m(self, a, speed, θspeed)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Now move and stop so that we can have a well defined actions  \u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspin_n(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# except KeyboardInterrupt:\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m#     print(\"Execution interrupted by user. Cleaning up...\")\u001b[39;00m\n\u001b[1;32m    221\u001b[0m reward, done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(a)\n",
      "File \u001b[0;32m~/git/turtlebot-as2/env/robot.py:227\u001b[0m, in \u001b[0;36mRobEnv.stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrobot\u001b[38;5;241m.\u001b[39mlinear\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.0\u001b[39m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrobot\u001b[38;5;241m.\u001b[39mangular\u001b[38;5;241m.\u001b[39mz \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.0\u001b[39m\n\u001b[0;32m--> 227\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspin_n\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msleep: time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m30\u001b[39m)\n",
      "File \u001b[0;32m~/git/turtlebot-as2/env/robot.py:198\u001b[0m, in \u001b[0;36mRobEnv.spin_n\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontroller\u001b[38;5;241m.\u001b[39mpublish(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrobot)\n\u001b[0;32m--> 198\u001b[0m     \u001b[43mros\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspin_once\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msleep: time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m30\u001b[39m)\n",
      "File \u001b[0;32m/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/__init__.py:206\u001b[0m, in \u001b[0;36mspin_once\u001b[0;34m(node, executor, timeout_sec)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     executor\u001b[38;5;241m.\u001b[39madd_node(node)\n\u001b[0;32m--> 206\u001b[0m     \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspin_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_sec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_sec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     executor\u001b[38;5;241m.\u001b[39mremove_node(node)\n",
      "File \u001b[0;32m/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/executors.py:751\u001b[0m, in \u001b[0;36mSingleThreadedExecutor.spin_once\u001b[0;34m(self, timeout_sec)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspin_once\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout_sec: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 751\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spin_once_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_sec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/executors.py:740\u001b[0m, in \u001b[0;36mSingleThreadedExecutor._spin_once_impl\u001b[0;34m(self, timeout_sec)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_spin_once_impl\u001b[39m(\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    737\u001b[0m     timeout_sec: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, TimeoutObject]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 740\u001b[0m         handler, entity, node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_ready_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_sec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_sec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ShutdownException:\n\u001b[1;32m    742\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/executors.py:723\u001b[0m, in \u001b[0;36mExecutor.wait_for_ready_callbacks\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cb_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_ready_callbacks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cb_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    725\u001b[0m     \u001b[38;5;66;03m# Generator ran out of work\u001b[39;00m\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cb_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/executors.py:620\u001b[0m, in \u001b[0;36mExecutor._wait_for_ready_callbacks\u001b[0;34m(self, timeout_sec, nodes, condition)\u001b[0m\n\u001b[1;32m    617\u001b[0m     waitable\u001b[38;5;241m.\u001b[39madd_to_wait_set(wait_set)\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Wait for something to become ready\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m \u001b[43mwait_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_nsec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_shutdown:\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ShutdownException()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resume = False\n",
    "\n",
    "if resume:\n",
    "    vqlearn = Qlearn.selfload(self_path='vQlearn_exp')\n",
    "    vqlearn.env = venv\n",
    "    vqlearn.episodes = 105 # extend sthe number of episodes\n",
    "\n",
    "# saving the object after each episode for retrieval in case of a crash \n",
    "%time vqlearn.interact(resume=resume, save_ep=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce587cf",
   "metadata": {},
   "source": [
    "# Model 2: Either policy gradient or value-based with non-linear function approximation\n",
    "\n",
    "I have also provided you with the ability to easily create a simple, fully connected neural network in the nnMRP class. You do not need to do a lot; the size of the input dictates the choice between a CNN-based model and a fully connected neural network. You may use either in your project, but using a traditional network, not a CNN, is easier and less time-consuming. The CNN-based one is for learning from the pixels of an input image; we are just using the laser reading in our project.\n",
    "\n",
    "I’ve also made slight changes to rl.rl.py to enable storing and retrieving an object (via pickle), which can be useful in case of unexpected crashes—something not uncommon in robotics.\n",
    "\n",
    "Please ensure you download turtlebot3.zip as well to guarantee the environment runs smoothly without any missing files. If you run into any issues, feel free to reach out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c2f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3080 Ti\n",
      "speed  =  2.0\n",
      "θspeed =  1.57\n",
      "state size(laser beams)= 1080\n",
      "--------------------- 易  DQN is being set up 易 -----------------------\n",
      "╭─────────────────────────────────────────────────────────────╮\n",
      "│               Model Architecture for Q net                  │\n",
      "├────┬────────────────────────────┬────────────┬──────────────┤\n",
      "│ Id │ Layer                      │ Parameters │ Trainable    │\n",
      "├────┼────────────────────────────┼────────────┼──────────────┤\n",
      "│  0 │ layers.0.weight            │      1,080 │ Yes          │\n",
      "│  1 │ layers.0.bias              │          3 │ Yes          │\n",
      "│  2 │ layers.1.weight            │          9 │ Yes          │\n",
      "│  3 │ layers.1.bias              │          3 │ Yes          │\n",
      "│  4 │ layers.2.weight            │      1,080 │ Yes          │\n",
      "│  5 │ layers.2.bias              │        360 │ Yes          │\n",
      "│  6 │ layers.3.weight            │      1,080 │ Yes          │\n",
      "│  7 │ layers.3.bias              │          3 │ Yes          │\n",
      "├────┴────────────────────────────┴────────────┴──────────────┤\n",
      "│ Total Parameters:         3,618 | Trainable:          3,618 │\n",
      "╰─────────────────────────────────────────────────────────────╯\n",
      "╭─────────────────────────────────────────────────────────────╮\n",
      "│               Model Architecture for Qn net                  │\n",
      "├────┬────────────────────────────┬────────────┬──────────────┤\n",
      "│ Id │ Layer                      │ Parameters │ Trainable    │\n",
      "├────┼────────────────────────────┼────────────┼──────────────┤\n",
      "│  0 │ layers.0.weight            │      1,080 │ Yes          │\n",
      "│  1 │ layers.0.bias              │          3 │ Yes          │\n",
      "│  2 │ layers.1.weight            │          9 │ Yes          │\n",
      "│  3 │ layers.1.bias              │          3 │ Yes          │\n",
      "│  4 │ layers.2.weight            │      1,080 │ Yes          │\n",
      "│  5 │ layers.2.bias              │        360 │ Yes          │\n",
      "│  6 │ layers.3.weight            │      1,080 │ Yes          │\n",
      "│  7 │ layers.3.bias              │          3 │ Yes          │\n",
      "├────┴────────────────────────────┴────────────┴──────────────┤\n",
      "│ Total Parameters:         3,618 | Trainable:          3,618 │\n",
      "╰─────────────────────────────────────────────────────────────╯\n",
      "Parameter containing:\n",
      "tensor([[ 0.0403,  0.0437, -0.0123,  ..., -0.0049, -0.0311,  0.0502],\n",
      "        [-0.0197, -0.0300, -0.0475,  ..., -0.0482, -0.0188, -0.0153],\n",
      "        [-0.0135,  0.0297,  0.0192,  ..., -0.0009, -0.0459, -0.0138]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.5193,  0.0376,  0.3747],\n",
      "        [ 0.5258,  0.3369, -0.2993],\n",
      "        [-0.5710,  0.2190,  0.3235]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0350, -0.3478,  0.4734],\n",
      "        [ 0.2465,  0.3823, -0.3905],\n",
      "        [ 0.3360, -0.3944,  0.5713],\n",
      "        ...,\n",
      "        [-0.5147, -0.1675,  0.4058],\n",
      "        [ 0.3515,  0.1787, -0.1694],\n",
      "        [-0.5389, -0.3102,  0.3010]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARN] [1745786143.466699011] [rcl.logging_rosout]: Publisher already registered for provided node name. If this is due to multiple nodes with the same name then all logs for that logger name will go out over the existing publisher. As soon as any node with that name is destructed it will unregister the publisher, preventing any further logs for that name from being published on the rosout topic.\n",
      "/home/danb/git/turtlebot-as2/rl/rlnn.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(s,    dtype=torch.float32),\n",
      "/home/danb/git/turtlebot-as2/rl/rlnn.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(sn,   dtype=torch.float32),\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[0;32m~/git/turtlebot-as2/rl/rl.py:193\u001b[0m, in \u001b[0;36mMRP.interact\u001b[0;34m(self, train, resume, save_ep, episodes, grid_img, **kw)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    192\u001b[0m rn,sn, a,an, done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(s,a, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt)  \u001b[38;5;66;03m# takes a step in env and store tarjectory if needed\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monline\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrn\u001b[49m\u001b[43m,\u001b[49m\u001b[43msn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43man\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# to learn online, pass a one step trajectory\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mΣr \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rn\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrn \u001b[38;5;241m=\u001b[39m rn\n",
      "File \u001b[0;32m~/git/turtlebot-as2/rl/rlnn.py:206\u001b[0m, in \u001b[0;36mDQN.online\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    203\u001b[0m Qn[dones] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    205\u001b[0m target \u001b[38;5;241m=\u001b[39m Qs\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m--> 206\u001b[0m target[inds, a] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mγ\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrn\u001b[49m\n\u001b[1;32m    207\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqN\u001b[38;5;241m.\u001b[39mfit(Qs, target)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_ \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_Qn \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "from env.robot import *\n",
    "import numpy as np\n",
    "from math import pi\n",
    "from time import sleep\n",
    "#from tqdm import tqdm\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "import termios\n",
    "import tty\n",
    "import select\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from robot_environment import *\n",
    "from rl.rlnn import *\n",
    "\n",
    "#ACTIONS - make the code easier to read\n",
    "FORWARDS = 1\n",
    "LEFT = 0\n",
    "RIGHT = 2\n",
    "\n",
    "#Really want it deterministic\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#GPU support?\n",
    "print(torch.cuda.is_available()) \n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0)) \n",
    "\n",
    "if not ros.ok():\n",
    "    ros.init()\n",
    "\n",
    "\n",
    "class nnRobEnv(vRobEnv):\n",
    "    def __init__(self, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.nF = len(self.scans)\n",
    "    \n",
    "    def s_(self):\n",
    "        max, min = self.max_range, self.min_range\n",
    "        normalised =  ((self.scans - min)/(max - min))\n",
    "        return torch.tensor(normalised, dtype=torch.float32)\n",
    "\n",
    "env = nnRobEnv()\n",
    "\n",
    "class cudaDQN(nnMDP):\n",
    "    def __init__(self, α=1e-4, t_Qn=1000, **kw):\n",
    "        print('--------------------- 易  cudaDQN is being set up 易 -----------------------')\n",
    "        super().__init__(**kw)\n",
    "        self.α = α\n",
    "        self.store = True\n",
    "        self.t_Qn = t_Qn\n",
    "        self.device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def greedy(self, s):\n",
    "        self.isamax = True\n",
    "        Qs = self.Q_(s)\n",
    "        Qs_np = Qs.detach().cpu().numpy() #WE NEED TO BRING THE DATA BACK FROM THE GPU for NUMPy\n",
    "        from numpy.random import choice\n",
    "        return choice((Qs_np == Qs_np.max()).nonzero()[0])\n",
    "\n",
    "    def online(self, *args):\n",
    "        if len(self.buffer) < self.nbatch:\n",
    "            return\n",
    "\n",
    "        (s, a, rn, sn, dones), inds = self.batch()\n",
    "\n",
    "        Qs = self.qN(s)\n",
    "        Qn = self.qNn(sn).detach()\n",
    "        Qn[dones] = 0\n",
    "\n",
    "        target = Qs.clone().detach()\n",
    "        target[inds, a] = self.γ * Qn.max(1).values + rn\n",
    "        loss = self.qN.fit(Qs, target)\n",
    "\n",
    "        if self.t_ % self.t_Qn == 0:\n",
    "            self.qNn.set_weights('Q', self.t_)\n",
    "            print(f'loss = {loss}')\n",
    "\n",
    "\n",
    "nnqlearn = cudaDQN(\n",
    "    env=env, \n",
    "    episodes=100, \n",
    "    α=1e-4,\n",
    "    ε=0.5, \n",
    "    dε=.99, \n",
    "    εmin=0.01, \n",
    "    γ=.95, \n",
    "    h1=3, \n",
    "    h2=3, \n",
    "    nF=env.nF,\n",
    "    nbuffer=5000, \n",
    "    nbatch=32, \n",
    "    endbatch=8,\n",
    "    t_Qn=100, \n",
    "    self_path='DQN_exp.pkl',\n",
    "    seed=1, \n",
    "    **demoGame())\n",
    "\n",
    "for layer in nnqlearn.qN.layers:\n",
    "    print(layer.weight)\n",
    "    # print(layer.bias)\n",
    "\n",
    "\n",
    "%time nnqlearn.interact(resume=False, save_ep=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cce715",
   "metadata": {},
   "source": [
    "# Framework bugs\n",
    "\n",
    "## Race for state not seen\n",
    "```plaintext\n",
    "'''\n",
    "---------------------------------------------------------------------------\n",
    "IndexError                                Traceback (most recent call last)\n",
    "File <timed eval>:1\n",
    "\n",
    "File ~/git/turtlebot-as2/rl/rl.py:192, in MRP.interact(self, train, resume, save_ep, episodes, grid_img, **kw)\n",
    "    189 self.t += 1\n",
    "    190 self.t_+= 1\n",
    "--> 192 rn,sn, a,an, done = self.step(s,a, self.t)  # takes a step in env and store tarjectory if needed\n",
    "    193 self.online(s, rn,sn, done, a,an) if train else None # to learn online, pass a one step trajectory\n",
    "    195 self.Σr += rn\n",
    "\n",
    "File ~/git/turtlebot-as2/rl/rl.py:153, in MRP.step_an(self, s, a, t)\n",
    "    151 if self.skipstep: return 0, None, None, None, True\n",
    "    152 sn, rn, done, _ = self.env.step(a)\n",
    "--> 153 an = self.policy(sn)\n",
    "    155 # we added s=s for compatibility with deep learning later\n",
    "    156 self.store_(s=s, a=a, rn=rn, sn=sn, an=an, done=done, t=t)\n",
    "\n",
    "File ~/git/turtlebot-as2/rl/rl.py:487, in MDP.<locals>.MDP.εgreedy(self, s)\n",
    "    484 if self.dε < 1: self.ε = max(self.εmin, self.ε*self.dε)              # exponential decay\n",
    "    485 if self.εT > 0: self.ε = max(self.εmin, self.ε0 - self.t_ / self.εT) # linear      decay\n",
    "--> 487 return self.greedy(s) if rand() > self.ε else randint(0, self.env.nA)\n",
    "\n",
    "File ~/git/turtlebot-as2/rl/rl.py:477, in MDP.<locals>.MDP.greedy(self, s)\n",
    "    474 # print(s)\n",
    "    475 # print(Qs)\n",
    "    476 if Qs.shape[0]==1: raise ValueError('something might be wrong number of actions ==1')\n",
    "--> 477 return choices(np.where(Qs==Qs.max())[0])[0]\n",
    "\n",
    "File /usr/lib/python3.10/random.py:519, in Random.choices(self, population, weights, cum_weights, k)\n",
    "    517     floor = _floor\n",
    "    518     n += 0.0    # convert to float for a small speed improvement\n",
    "--> 519     return [population[floor(random() * n)] for i in _repeat(None, k)]\n",
    "    520 try:\n",
    "    521     cum_weights = list(_accumulate(weights))\n",
    "\n",
    "File /usr/lib/python3.10/random.py:519, in <listcomp>(.0)\n",
    "    517     floor = _floor\n",
    "    518     n += 0.0    # convert to float for a small speed improvement\n",
    "--> 519     return [population[floor(random() * n)] for i in _repeat(None, k)]\n",
    "    520 try:\n",
    "    521     cum_weights = list(_accumulate(weights))\n",
    "\n",
    "IndexError: index 0 is out of bounds for axis 0 with size 0\n",
    "\n",
    "FIXED BY EDITING rl.py\n",
    "\n",
    "#------------------------------------- add some more policies types 易-------------------------------\n",
    "        # useful for inheritance, gives us a vector of actions values\n",
    "        def Q_(self, s=None, a=None):\n",
    "\n",
    "            #Originally return self.Q[s] if s is not None else self.Q\n",
    "\n",
    "            if s is None:\n",
    "                return self.Q  \n",
    "            \n",
    "            #just initialise to 0 for now, not sure how to handle this.\n",
    "            if s not in self.Q:\n",
    "                self.Q[s] = np.zeros(self.env.nA) \n",
    "            \n",
    "            return self.Q[s]\n",
    "\n",
    "'''\n",
    "```\n",
    "\n",
    "## S_ instead of s_ in rl.py\n",
    "```plaintext\n",
    "--------------------------------------------------------------------------\n",
    "AttributeError                            Traceback (most recent call last)\n",
    "File <timed eval>:1\n",
    "\n",
    "File ~/git/turtlebot-as2/rl/rl.py:182, in MRP.interact(self, train, resume, save_ep, episodes, grid_img, **kw)\n",
    "    179 done = False\n",
    "    180 #print(self.ep)\n",
    "    181 # initial step\n",
    "--> 182 s,a = self.step_0()\n",
    "    183 self.step0()                                    # user defined init of each episode\n",
    "    184 # an episode is a set of steps, interact and learn from experience, online or offline.\n",
    "\n",
    "File ~/git/turtlebot-as2/rl/rl.py:134, in MRP.step_0(self)\n",
    "    132 def step_0(self):\n",
    "    133     s = self.env.reset()                                 # set env/agent to the start position\n",
    "--> 134     a = self.policy(s)\n",
    "    135     return s,a\n",
    "\n",
    "File ~/git/turtlebot-as2/rl/rl.py:499, in MDP.<locals>.MDP.εgreedy(self, s)\n",
    "    496 if self.dε < 1: self.ε = max(self.εmin, self.ε*self.dε)              # exponential decay\n",
    "    497 if self.εT > 0: self.ε = max(self.εmin, self.ε0 - self.t_ / self.εT) # linear      decay\n",
    "--> 499 return self.greedy(s) if rand() > self.ε else randint(0, self.env.nA)\n",
    "\n",
    "File ~/git/turtlebot-as2/rl/rl.py:485, in MDP.<locals>.MDP.greedy(self, s)\n",
    "    483 self.isamax = True\n",
    "    484 # instead of returning np.argmax(Q[s]) get all max actions and return one of the max actions randomly\n",
    "--> 485 Qs = self.Q_(s)\n",
    "    486 # print(s)\n",
    "    487 # print(Qs)\n",
    "    488 if Qs.shape[0]==1: raise ValueError('something might be wrong number of actions ==1')\n",
    "\n",
    "File ~/git/turtlebot-as2/rl/rlln.py:149, in vMDP.Q_(self, s, a)\n",
    "    145 def Q_(self, s=None, a=None):\n",
    "    146     #print(f\"{s.shape}, {a}\")\n",
    "    148     W = self.W if a is None else self.W[a]\n",
    "--> 149     return W.dot(s) if s is not None else np.matmul(W, self.env.S_()).T\n",
    "\n",
    "AttributeError: 'vRobEnv' object has no attribute 'S_'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6066b",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Núñez, P., Vazquez-Martin, R., Bandera, A., and Romero-Gonzalez, C. (2015) ‘Feature extraction from laser scan data based on curvature estimation for mobile robotics’, *Robotics and Autonomous Systems*, 70, pp. 103–114. Available at: [https://robolab.unex.es/wp-content/papercite-data/pdf/feature-extraction-from-laser.pdf](https://robolab.unex.es/wp-content/publicaciones/2006/Nunez%20Trujillo,%20Vazquez-Martin,%20del%20Toro,%20Bandera%20%7C%20Feature%20extraction%20from%20laser%20scan%20data%20based%20on%20curvature%20estimation%20for%20mobile%20robotics.pdf) (Accessed: 26 April 2025).\n",
    "\n",
    "- Ramos, J., Rocha, R., and Dias, J. (2022) ‘Efficient approach for extracting high-level B-spline features from laser scan data’, *Sensors*, 22(24), 9737. Available at: [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9737135/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9737135/) (Accessed: 26 April 2025).\n",
    "\n",
    "- Shen, S., Michael, N., and Kumar, V. (2012) ‘Method for corner feature extraction from laser scan data’, *ResearchGate*. Available at: [https://www.researchgate.net/publication/288577925_Method_for_corner_feature_extraction_from_laser_scan_data](https://www.researchgate.net/publication/288577925_Method_for_corner_feature_extraction_from_laser_scan_data) (Accessed: 26 April 2025).\n",
    "\n",
    "- Stack Overflow (2019) ‘How can I detect the corner from 2D point cloud or LiDAR scanned data?’, *Stack Overflow*. Available at: [https://stackoverflow.com/questions/59049990/how-can-i-detect-the-corner-from-2d-point-cloud-or-lidar-scanned-data](https://stackoverflow.com/questions/59049990/how-can-i-detect-the-corner-from-2d-point-cloud-or-lidar-scanned-data) (Accessed: 26 April 2025).\n",
    "\n",
    "\n",
    "- CETI. (n.d.) *Simulation Speed in ROS/Gazebo*. Available at: [https://ceti.pages.st.inf.tu-dresden.de/robotics/howtos/SimulationSpeed.html](https://ceti.pages.st.inf.tu-dresden.de/robotics/howtos/SimulationSpeed.html) (Accessed: 26 April 2025).\n",
    "\n",
    "- Furrer, F., Wermelinger, M., Naegeli, T., et al. (2021) ‘Dynamics and Control of Quadrotor UAVs: A Survey’, *IEEE Transactions on Robotics*, 37(5), pp. 1381–1400. Available at: [https://ieeexplore.ieee.org/document/9453594](https://ieeexplore.ieee.org/document/9453594) (Accessed: 26 April 2025).\n",
    "\n",
    "- Perez-Perez, J., Jimenez, F. and Mata, M. (2023) ‘An Overview of Reinforcement Learning in Autonomous Driving: Fundamentals, Challenges, and Applications’, *Applied Sciences*, 13(12), p. 7202. Available at: [https://www.mdpi.com/2076-3417/13/12/7202](https://www.mdpi.com/2076-3417/13/12/7202) (Accessed: 26 April 2025).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbb3939",
   "metadata": {},
   "source": [
    "# Appendicies\n",
    "\n",
    "## Cool links / interesting reading: \n",
    "- https://github.com/hello-robot/stretch_ros/blob/master/stretch_funmap/README.md\n",
    "- https://arxiv.org/pdf/2502.20607\n",
    "\n",
    "## Miscelaneous Notes\n",
    "\n",
    "### Setting up ROS\n",
    "- https://emanual.robotis.com/docs/en/platform/turtlebot3/sbc_setup/\n",
    "- https://ros2-industrial-workshop.readthedocs.io/en/latest/_source/navigation/ROS2-Turtlebot.html\n",
    "- https://emanual.robotis.com/docs/en/platform/turtlebot3/navigation/\n",
    "- https://emanual.robotis.com/docs/en/platform/turtlebot3/bringup/#bringup\n",
    "\n",
    "### Multicast traffic (for DDS) through Windows FW to WSL2:\n",
    "- https://eprosima-dds-router.readthedocs.io/en/latest/rst/examples/repeater_example.html#execute-example\n",
    "- New-NetFirewallRule -Name 'WSL' -DisplayName 'WSL' -InterfaceAlias 'vEthernet (WSL (Hyper-V firewall))' -Direction Inbound -Action Allow\n",
    "- New-NetIPAddress -InterfaceAlias 'vEthernet (WSL (Hyper-V firewall))' -IPAddress '192.168.1.217' -PrefixLength 24\n",
    "- https://github.com/DanielBryars/multicast-test.git\n",
    "\n",
    "### VM\n",
    "- https://labs.azure.com/virtualmachines?feature_vnext=true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
